{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from collections import deque\n",
    "import timeit\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from baselines.logger import HumanOutputFormat\n",
    "\n",
    "from level_replay import utils\n",
    "from level_replay.algo.dqn import DQN\n",
    "from level_replay.algo.policy import LAP_DDQN\n",
    "from level_replay.algo.buffer import PrioritizedBuffer\n",
    "from level_replay.model import model_for_env_name\n",
    "from level_replay.storage import RolloutStorage\n",
    "from level_replay.file_writer import FileWriter\n",
    "from level_replay.envs import make_lr_venv\n",
    "from level_replay.arguments import parser\n",
    "from test import evaluate\n",
    "from tqdm import trange\n",
    "\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.start_timesteps = 10000\n",
    "        self.train_freq = 4\n",
    "        self.eval_freq = 300\n",
    "        self.num_actions = 15\n",
    "        self.state_dim = (3, 64, 64)\n",
    "        self.cuda = False\n",
    "        self.device = torch.device(\"cuda:0\" if self.cuda else \"cpu\")\n",
    "        self.discount = 0.99\n",
    "        self.adam_eps = 1.5e-4\n",
    "        self.learning_rate = 2.5e-4\n",
    "        self.optimizer = \"Adam\"\n",
    "        self.optimizer_parameters = {'lr': self.learning_rate,  'eps': self.adam_eps}\n",
    "        self.polyak_target_update = False\n",
    "        self.target_update_frequency = 8e3\n",
    "        self.tau = 0.005\n",
    "        self.initial_eps = 1\n",
    "        self.end_eps = 0.001\n",
    "        self.eps_decay_period = 25e4\n",
    "        self.eval_eps = 0.001\n",
    "        self.alpha = 0.6\n",
    "        self.min_priority=1e-2\n",
    "        self.V_min = 0\n",
    "        self.V_max = 40\n",
    "        self.batch_size = 32\n",
    "        self.multi_step = 3\n",
    "        self.norm_clip = 10\n",
    "        self.atoms = 51\n",
    "        self.hidden_size = 32\n",
    "        self.noisy_std = 0.1\n",
    "        self.T_max = int(50e4)\n",
    "        self.max_episode_length = int(108e3)\n",
    "        self.V_min = -10.\n",
    "        self.V_max = 10.\n",
    "        self.learn_start = int(2e2)#20e3)\n",
    "        self.evaluation_interval = 100000\n",
    "        self.evaluation_episodes = 10\n",
    "        self.evaluation_size = 500\n",
    "        self.render = False\n",
    "        self.checkpoint_interval = 0\n",
    "        self.memory_capacity = int(1e6)\n",
    "        self.replay_frequency = 4\n",
    "        self.norm_clip = 10\n",
    "        self.batch_size = 32\n",
    "        self.learning_rate = 0.0000625\n",
    "        self.reward_clip = 1\n",
    "        self.target_update = int(8e3)\n",
    "        self.discount = 0.99\n",
    "        self.atoms = 51\n",
    "        self.hidden_size = 32\n",
    "        self.noisy_std = 0.1\n",
    "        self.model = None\n",
    "        self.cuda = False\n",
    "        self.device = torch.device(\"cuda:0\" if self.cuda else \"cpu\")\n",
    "        self.capacity = int(1e6)\n",
    "        self.history_length = 4\n",
    "        self.multi_step = 3\n",
    "        self.priority_weight = 0.4  # Initial importance sampling weight Î², annealed to 1 over course of training\n",
    "        self.priority_exponent = 0.5\n",
    "        self.t = 0  # Internal episode timestep counter\n",
    "        self.num_processes = 1\n",
    "        self.env_name = 'bigfish'\n",
    "        self.distribution_mode = 'easy'\n",
    "        self.paint_vel_info = False\n",
    "        self.start_level = 0\n",
    "        self.no_ret_normalization = False\n",
    "        self.eps=1e-05\n",
    "        self.alpha=0.99\n",
    "        self.arch='large'\n",
    "        self.clip_param=0.2\n",
    "        self.disable_checkpoint=False\n",
    "        self.distribution_mode='easy' \n",
    "        self.entropy_coef=0.01\n",
    "        self.final_num_test_seeds=1000\n",
    "        self.full_train_distribution=False \n",
    "        self.gae_lambda=0.95\n",
    "        self.gamma=0.999\n",
    "        self.hidden_size=256\n",
    "        self.level_replay_alpha=1.0\n",
    "        self.level_replay_eps=0.05\n",
    "        self.level_replay_nu=0.5\n",
    "        self.level_replay_rho=1.0\n",
    "        self.level_replay_schedule='proportionate'\n",
    "        self.level_replay_score_transform='rank'\n",
    "        self.level_replay_strategy='value_l1'\n",
    "        self.level_replay_temperature=0.1\n",
    "        self.log_dir='~/logs/rainbow/'\n",
    "        self.log_interval=1\n",
    "        #self.lr=0.0005\n",
    "        self.max_grad_norm=0.5\n",
    "        self.no_cuda=False\n",
    "        self.num_env_steps=25000000.0\n",
    "        self.num_mini_batch=8\n",
    "        self.num_steps=256\n",
    "        self.num_test_seeds=10\n",
    "        self.num_train_seeds=200\n",
    "        self.paint_vel_info=False\n",
    "        self.ppo_epoch=3\n",
    "        self.save_interval=60\n",
    "        self.seed=1\n",
    "        self.seed_path=None\n",
    "        self.staleness_coef=0.1 \n",
    "        self.staleness_temperature=1.0\n",
    "        self.staleness_transform='power'\n",
    "        self.start_level=0\n",
    "        self.value_loss_coef=0.5, \n",
    "        self.verbose=False\n",
    "        self.weight_log_interval=1\n",
    "        self.xpid='latest'\n",
    "        self.num_processes=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy(policy, eval_episodes=10):\n",
    "    eval_envs, level_sampler = make_lr_venv(\n",
    "        num_envs=args.num_processes, env_name=args.env_name,\n",
    "        seeds=seeds, device=args.device,\n",
    "        num_levels=num_levels, start_level=start_level,\n",
    "        no_ret_normalization=args.no_ret_normalization,\n",
    "        distribution_mode=args.distribution_mode,\n",
    "        paint_vel_info=args.paint_vel_info,\n",
    "        level_sampler_args=level_sampler_args)\n",
    "    \n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state, level_seeds = eval_envs.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, q = policy.select_action(state, eval=True)\n",
    "            state, _, done, infos = eval_envs.step(action)\n",
    "            for info in infos:\n",
    "                if 'episode' in info.keys():\n",
    "                    avg_reward += info['episode']['r']\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward\n",
    "\n",
    "def multi_step_reward(rewards, gamma):\n",
    "    ret = 0.\n",
    "    for idx, reward in enumerate(rewards):\n",
    "        ret += reward * (gamma ** idx)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "start_level = 0\n",
    "\n",
    "def generate_seeds(num_seeds, base_seed=0):\n",
    "    return [base_seed + i for i in range(num_seeds)]\n",
    "\n",
    "seeds = generate_seeds(args.num_train_seeds)\n",
    "\n",
    "num_levels = 1\n",
    "level_sampler_args = dict(\n",
    "    num_actors=args.num_processes,\n",
    "    strategy=args.level_replay_strategy,\n",
    "    replay_schedule=args.level_replay_schedule,\n",
    "    score_transform=args.level_replay_score_transform,\n",
    "    temperature=args.level_replay_temperature,\n",
    "    eps=args.level_replay_eps,\n",
    "    rho=args.level_replay_rho,\n",
    "    nu=args.level_replay_nu, \n",
    "    alpha=args.level_replay_alpha,\n",
    "    staleness_coef=args.staleness_coef,\n",
    "    staleness_transform=args.staleness_transform,\n",
    "    staleness_temperature=args.staleness_temperature\n",
    ")\n",
    "envs, level_sampler = make_lr_venv(\n",
    "    num_envs=args.num_processes, env_name=args.env_name,\n",
    "    seeds=seeds, device=args.device,\n",
    "    num_levels=num_levels, start_level=start_level,\n",
    "    no_ret_normalization=args.no_ret_normalization,\n",
    "    distribution_mode=args.distribution_mode,\n",
    "    paint_vel_info=args.paint_vel_info,\n",
    "    level_sampler_args=level_sampler_args)\n",
    "\n",
    "prioritized = True\n",
    "replay_buffer = PrioritizedBuffer(\n",
    "    args.state_dim,\n",
    "    args.batch_size, \n",
    "    int(1e4),#args.memory_capacity, \n",
    "    args.device, True\n",
    ")\n",
    "\n",
    "agent = LAP_DDQN(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seeds(num_seeds, base_seed=0):\n",
    "    return [base_seed + i for i in range(num_seeds)]\n",
    "\n",
    "seeds = generate_seeds(args.num_train_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_seeds = torch.zeros(args.num_processes)\n",
    "if level_sampler:\n",
    "    state, level_seeds = envs.reset()\n",
    "level_seeds.unsqueeze(-1)\n",
    "episode_rewards = deque(maxlen=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_start = True\n",
    "episode_reward = 0\n",
    "episode_timesteps = 0\n",
    "episode_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_deque = deque(maxlen=args.multi_step)\n",
    "reward_deque = deque(maxlen=args.multi_step)\n",
    "action_deque = deque(maxlen=args.multi_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 10.4\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 11.6\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 1.8\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 14.8\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 3.7\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 1.0\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 2.0\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 4.0\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 1.7\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 7.6\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 5.8\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 11.6\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 3.5\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 1.6\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 2.0\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 5.0\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 2.0\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 2.0\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 1.9\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 4.7\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 4.9\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 2.0\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 1.6\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 1.6\n",
      "---------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9d6e2d00e704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Train agent after collecting sufficient data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_timesteps\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_update\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PLEXR/level_replay/algo/policy.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, replay_buffer)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mlog_p1s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PLEXR/level_replay/algo/buffer.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m         batch = (\n\u001b[1;32m     47\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in range(int(args.T_max)):\n",
    "\n",
    "    episode_timesteps += 1\n",
    "    \n",
    "    if t % args.replay_frequency == 0:\n",
    "        agent.Q.reset_noise()\n",
    "\n",
    "    #if args.train_behavioral:\n",
    "    if t < args.start_timesteps:\n",
    "        action = torch.LongTensor([envs.action_space.sample()]).reshape(-1, 1)\n",
    "    else:\n",
    "        action, q = agent.select_action(state)\n",
    "\n",
    "    # Perform action and log results\n",
    "    next_state, reward, done, infos = envs.step(action)\n",
    "    state_deque.append(state)\n",
    "    reward_deque.append(reward)\n",
    "    action_deque.append(action)\n",
    "\n",
    "    if len(state_deque) == args.multi_step or done:\n",
    "        n_reward = multi_step_reward(reward_deque, args.gamma)\n",
    "        n_state = state_deque[0]\n",
    "        n_action = action_deque[0]\n",
    "        replay_buffer.add(n_state, n_action, next_state, n_reward, np.float32(done), done, episode_start)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    episode_start = False\n",
    "\n",
    "    # Only consider \"done\" if episode terminates due to failure condition\n",
    "    #done_float = float(done) if episode_timesteps < envs._max_episode_steps else 0\n",
    "\n",
    "    # For atari, info[0] = clipped reward, info[1] = done_float\n",
    "    for i, info in enumerate(infos):\n",
    "        if 'bad_transition' in info.keys():\n",
    "            print(\"Bad transition\")\n",
    "        #print(info)\n",
    "        if 'episode' in info.keys():\n",
    "            episode_rewards.append(info['episode']['r'])\n",
    "        if level_sampler:\n",
    "            level_seeds[i] = info['level_seed']\n",
    "\n",
    "    # Train agent after collecting sufficient data\n",
    "    if t >= args.start_timesteps and (t + 1) % args.train_freq == 0:\n",
    "        loss = agent.train(replay_buffer)\n",
    "        \n",
    "\n",
    "        # Evaluate episode\n",
    "        if (t + 1) % args.eval_freq == 0:\n",
    "            evaluations.append(eval_policy(agent))\n",
    "            #np.save(f\"./results/{setting}.npy\", evaluations)\n",
    "            \n",
    "    if t % args.target_update == 0:\n",
    "            agent.copy_target_update()\n",
    "\n",
    "    if done:\n",
    "        # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
    "        #print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward}\")\n",
    "        # Reset environment\n",
    "        state, level_seeds = envs.reset()\n",
    "        done = False\n",
    "        episode_start = True\n",
    "        episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "        episode_num += 1\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
